\begin{thebibliography}{33}
\newcommand{\enquote}[1]{``#1''}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Adeodato \& Melo(2016)}]{adeodato2016equivalence}
\textsc{Adeodato}, P.~J. \& S.~B. \textsc{Melo} (2016): \enquote{On the
  equivalence between kolmogorov-smirnov and roc curve metrics for binary
  classification.}
\newblock \emph{arXiv preprint arXiv:1606.00496} .

\bibitem[{Bol{\'o}n-Canedo \emph{et~al.}(2015)Bol{\'o}n-Canedo,
  S{\'a}nchez-Maro{\~n}o, \& Alonso-Betanzos}]{bolon2015feature}
\textsc{Bol{\'o}n-Canedo}, V., N.~\textsc{S{\'a}nchez-Maro{\~n}o}, \&
  A.~\textsc{Alonso-Betanzos} (2015): \emph{Feature selection for
  high-dimensional data}.
\newblock Springer.

\bibitem[{Boughorbel \emph{et~al.}(2017)Boughorbel, Jarray, \&
  El-Anbari}]{boughorbel2017optimal}
\textsc{Boughorbel}, S., F.~\textsc{Jarray}, \& M.~\textsc{El-Anbari} (2017):
  \enquote{Optimal classifier for imbalanced data using matthews correlation
  coefficient metric.}
\newblock \emph{PloS one} \textbf{12(6)}: p. e0177678.

\bibitem[{Brabec \emph{et~al.}(2020)Brabec, Kom{\'a}rek, Franc, \&
  Machlica}]{brabec2020model}
\textsc{Brabec}, J., T.~\textsc{Kom{\'a}rek}, V.~\textsc{Franc}, \&
  L.~\textsc{Machlica} (2020): \enquote{On model evaluation under non-constant
  class imbalance.}
\newblock In \enquote{Computational Science--ICCS 2020: 20th International
  Conference, Amsterdam, The Netherlands, June 3--5, 2020, Proceedings, Part IV
  20,} pp. 74--87. Springer.

\bibitem[{Brownlee(2020)}]{Brownlee2020}
\textsc{Brownlee}, J. (2020): \enquote{Recursive feature elimination (rfe) for
  feature selection in python.}
\newblock Retrieved April 28, 2023.

\bibitem[{Brownlee(2021)}]{brownlee2021failure}
\textsc{Brownlee}, J. (2021): \enquote{Failure of classification accuracy for
  imbalanced class distributions.}
\newblock \emph{MachineLearningMastery.com} .

\bibitem[{Chicco \& Jurman(2020)}]{chicco2020advantages}
\textsc{Chicco}, D. \& G.~\textsc{Jurman} (2020): \enquote{The advantages of
  the matthews correlation coefficient (mcc) over f1 score and accuracy in
  binary classification evaluation.}
\newblock \emph{BMC genomics} \textbf{21}: pp. 1--13.

\bibitem[{Cichosz(2014)}]{cichosz2014data}
\textsc{Cichosz}, P. (2014): \emph{Data mining algorithms: explained using R}.
\newblock John Wiley \& Sons.

\bibitem[{Comotto(2022)}]{comotto2022evaluation}
\textsc{Comotto}, F. (2022): \enquote{Evaluation metrics: Leave your comfort
  zone and try mcc and brier score.}
\newblock Medium.
\newblock Retrieved April 30, 2023.

\bibitem[{Dembla(2020)}]{dembla2020intuition}
\textsc{Dembla}, G. (2020): \enquote{Intuition behind log-loss score.}
\newblock Medium.
\newblock Retrieved April 30, 2023.

\bibitem[{Gauhar(2020)}]{gauhar2020decision}
\textsc{Gauhar}, N. (2020): \enquote{Decision tree: A classification
  algorithm.}
\newblock
  \url{https://learnwithgauhar.com/decision-tree-a-classification-algorithm/}.
\newblock Accessed on April 30, 2023.

\bibitem[{Han \emph{et~al.}(2011)Han, Kamber, \& Pei}]{han2011data}
\textsc{Han}, J., M.~\textsc{Kamber}, \& J.~\textsc{Pei} (2011): \emph{Data
  Mining: Concepts and Techniques}.
\newblock Morgan Kaufmann, 3rd edition.

\bibitem[{He \emph{et~al.}(2008)He, Bai, Garcia, \& Li}]{adasynhaibo}
\textsc{He}, H., Y.~\textsc{Bai}, E.~A. \textsc{Garcia}, \& S.~\textsc{Li}
  (2008): \enquote{Adasyn: Adaptive synthetic sampling approach for imbalanced
  learning.}
\newblock In \enquote{2008 IEEE International Joint Conference on Neural
  Networks (IEEE World Congress on Computational Intelligence),} pp.
  1322--1328.

\bibitem[{He \& Ma(2013)}]{ma2013imbalanced}
\textsc{He}, H. \& Y.~\textsc{Ma} (2013): \emph{Imbalanced Learning:
  Foundations, Algorithms, and Applications}.
\newblock Wiley-IEEE Press.

\bibitem[{Igareta(2021)}]{igareta2021strat}
\textsc{Igareta}, A. (2021): \enquote{Stratified sampling: You may have been
  splitting your dataset all wrong.}

\bibitem[{Japkowicz \& Shah(2011)}]{japkowicz2011evaluating}
\textsc{Japkowicz}, N. \& M.~\textsc{Shah} (2011): \emph{Evaluating learning
  algorithms: a classification perspective}.
\newblock Cambridge University Press.

\bibitem[{Kaushik(2016)}]{kaushik2016introduction}
\textsc{Kaushik}, S. (2016): \enquote{Introduction to feature selection methods
  with an example (or how to select the right variables?).}
\newblock Accessed: April 30, 2023.

\bibitem[{Kornbrot(2014)}]{kornbrot2014point}
\textsc{Kornbrot}, D. (2014): \enquote{Point biserial correlation.}
\newblock \emph{Wiley StatsRef: Statistics Reference Online} .

\bibitem[{Leskovec \emph{et~al.}(2020)Leskovec, Rajaraman, \&
  Ullman}]{leskovec2020mining}
\textsc{Leskovec}, J., A.~\textsc{Rajaraman}, \& J.~D. \textsc{Ullman} (2020):
  \emph{Mining of massive data sets}.
\newblock Cambridge university press.

\bibitem[{Malley \emph{et~al.}(2011)Malley, Kruppa, Dasgupta, Malley, \&
  Ziegler}]{randomforestmalley}
\textsc{Malley}, J., J.~\textsc{Kruppa}, A.~\textsc{Dasgupta},
  K.~\textsc{Malley}, \& A.~\textsc{Ziegler} (2011): \enquote{Probability
  machines consistent probability estimation using nonparametric learning
  machines.}
\newblock \emph{Methods of information in medicine} \textbf{51}: pp. 74--81.

\bibitem[{Mucherino \emph{et~al.}(2009)Mucherino, Papajorgji, \&
  Pardalos}]{mucherino2009data}
\textsc{Mucherino}, A., P.~\textsc{Papajorgji}, \& P.~M. \textsc{Pardalos}
  (2009): \emph{Data mining in agriculture}, volume~34.
\newblock Springer Science \& Business Media.

\bibitem[{Navas-Palencia(2020)}]{navas2020optimal}
\textsc{Navas-Palencia}, G. (2020): \enquote{Optimal binning: mathematical
  programming formulation.}
\newblock \emph{arXiv preprint arXiv:2001.08025} .

\bibitem[{Newson(2002)}]{newson2002parameters}
\textsc{Newson}, R. (2002): \enquote{Parameters behind “nonparametric”
  statistics: Kendall's tau, somers’ d and median differences.}
\newblock \emph{The Stata Journal} \textbf{2(1)}: pp. 45--64.

\bibitem[{Newson(2014)}]{newson2014interpretation}
\textsc{Newson}, R.~B. (2014): \enquote{Interpretation of somers' d under four
  simple models.}

\bibitem[{Nian(2018)}]{nian2018introduction}
\textsc{Nian}, R. (2018): \enquote{An introduction to {ADASYN} (with code!).}
\newblock Medium.
\newblock Retrieved on May 2, 2023 from
  \url{https://medium.com/@ruinian/an-introduction-to-adasyn-with-code-1383a5ece7aa}.

\bibitem[{Provost \& Fawcett(2013)}]{provost2013data}
\textsc{Provost}, F. \& T.~\textsc{Fawcett} (2013): \emph{Data Science for
  Business: What you need to know about data mining and data-analytic
  thinking}.
\newblock " O'Reilly Media, Inc.".

\bibitem[{Ribeiro \emph{et~al.}(2016)Ribeiro, Singh, \&
  Guestrin}]{ribeiro2016should}
\textsc{Ribeiro}, M.~T., S.~\textsc{Singh}, \& C.~\textsc{Guestrin} (2016):
  \enquote{" why should i trust you?" explaining the predictions of any
  classifier.}
\newblock In \enquote{Proceedings of the 22nd ACM SIGKDD international
  conference on knowledge discovery and data mining,} pp. 1135--1144.

\bibitem[{{scikit-learn}(n.d.)}]{sfs}
\textsc{{scikit-learn}} (n.d.): \enquote{Sequentialfeatureselector.}
\newblock
  \url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector}.
\newblock Retrieved April 28, 2023.

\bibitem[{Verma(2020)}]{Verma2020}
\textsc{Verma}, V. (2020): \enquote{A comprehensive guide to feature selection
  using wrapper methods in python.}
\newblock
  \url{https://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/}.
\newblock Retrieved April 30, 2023.

\bibitem[{Verma(2021)}]{Verma2021}
\textsc{Verma}, Y. (2021): \enquote{A complete guide to sequential feature
  selection.}
\newblock
  \url{https://analyticsindiamag.com/a-complete-guide-to-sequential-feature-selection/}.
\newblock Retrieved April 28, 2023.

\bibitem[{Wendler \& Gr{\"o}ttrup(2021)}]{wendler2021data}
\textsc{Wendler}, T. \& S.~\textsc{Gr{\"o}ttrup} (2021): \emph{Data Mining with
  SPSS Modeler: Theory, Exercises and Solutions}.
\newblock Cham: Springer.

\bibitem[{Witten \emph{et~al.}(2011)Witten, Frank, Hall, \&
  Pal}]{witten2011data}
\textsc{Witten}, I.~H., E.~\textsc{Frank}, M.~\textsc{Hall}, \& C.~\textsc{Pal}
  (2011): \emph{Data Mining: Practical Machine Learning Tools and Techniques}.
\newblock Morgan Kaufmann, 4th edition.

\bibitem[{Zeng(2014)}]{zeng2014necessary}
\textsc{Zeng}, G. (2014): \enquote{A necessary condition for a good binning
  algorithm in credit scoring.}
\newblock \emph{Applied Mathematical Sciences} \textbf{8(65)}: pp. 3229--3242.

\end{thebibliography}
