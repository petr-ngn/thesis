\begin{thebibliography}{21}
\newcommand{\enquote}[1]{``#1''}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Adeodato \& Melo(2016)}]{adeodato2016equivalence}
\textsc{Adeodato}, P.~J. \& S.~B. \textsc{Melo} (2016): \enquote{On the
  equivalence between kolmogorov-smirnov and roc curve metrics for binary
  classification.}
\newblock \emph{arXiv preprint arXiv:1606.00496} .

\bibitem[{Boughorbel \emph{et~al.}(2017)Boughorbel, Jarray, \&
  El-Anbari}]{boughorbel2017optimal}
\textsc{Boughorbel}, S., F.~\textsc{Jarray}, \& M.~\textsc{El-Anbari} (2017):
  \enquote{Optimal classifier for imbalanced data using matthews correlation
  coefficient metric.}
\newblock \emph{PloS one} \textbf{12(6)}: p. e0177678.

\bibitem[{Brabec \emph{et~al.}(2020)Brabec, Kom{\'a}rek, Franc, \&
  Machlica}]{brabec2020model}
\textsc{Brabec}, J., T.~\textsc{Kom{\'a}rek}, V.~\textsc{Franc}, \&
  L.~\textsc{Machlica} (2020): \enquote{On model evaluation under non-constant
  class imbalance.}
\newblock In \enquote{Computational Science--ICCS 2020: 20th International
  Conference, Amsterdam, The Netherlands, June 3--5, 2020, Proceedings, Part IV
  20,} pp. 74--87. Springer.

\bibitem[{Brownlee(2021)}]{brownlee2021failure}
\textsc{Brownlee}, J. (2021): \enquote{Failure of classification accuracy for
  imbalanced class distributions.}
\newblock \emph{MachineLearningMastery.com} .

\bibitem[{Chicco \& Jurman(2020)}]{chicco2020advantages}
\textsc{Chicco}, D. \& G.~\textsc{Jurman} (2020): \enquote{The advantages of
  the matthews correlation coefficient (mcc) over f1 score and accuracy in
  binary classification evaluation.}
\newblock \emph{BMC genomics} \textbf{21}: pp. 1--13.

\bibitem[{Cichosz(2014)}]{cichosz2014data}
\textsc{Cichosz}, P. (2014): \emph{Data mining algorithms: explained using R}.
\newblock John Wiley \& Sons.

\bibitem[{Dembla(2020)}]{dembla2020intuition}
\textsc{Dembla}, G. (2020): \enquote{Intuition behind log-loss score.}
\newblock Medium.
\newblock Retrieved April 30, 2023.

\bibitem[{Gauhar(2020)}]{gauhar2020decision}
\textsc{Gauhar}, N. (2020): \enquote{Decision tree: A classification
  algorithm.}
\newblock
  \url{https://learnwithgauhar.com/decision-tree-a-classification-algorithm/}.
\newblock Accessed on April 30, 2023.

\bibitem[{Han \emph{et~al.}(2011)Han, Kamber, \& Pei}]{han2011data}
\textsc{Han}, J., M.~\textsc{Kamber}, \& J.~\textsc{Pei} (2011): \emph{Data
  Mining: Concepts and Techniques}.
\newblock Morgan Kaufmann, 3rd edition.

\bibitem[{He \emph{et~al.}(2008)He, Bai, Garcia, \& Li}]{adasynhaibo}
\textsc{He}, H., Y.~\textsc{Bai}, E.~A. \textsc{Garcia}, \& S.~\textsc{Li}
  (2008): \enquote{Adasyn: Adaptive synthetic sampling approach for imbalanced
  learning.}
\newblock In \enquote{2008 IEEE International Joint Conference on Neural
  Networks (IEEE World Congress on Computational Intelligence),} pp.
  1322--1328.

\bibitem[{Igareta(2021)}]{igareta2021strat}
\textsc{Igareta}, A. (2021): \enquote{Stratified sampling: You may have been
  splitting your dataset all wrong.}

\bibitem[{Japkowicz \& Shah(2011)}]{japkowicz2011evaluating}
\textsc{Japkowicz}, N. \& M.~\textsc{Shah} (2011): \emph{Evaluating learning
  algorithms: a classification perspective}.
\newblock Cambridge University Press.

\bibitem[{Kornbrot(2014)}]{kornbrot2014point}
\textsc{Kornbrot}, D. (2014): \enquote{Point biserial correlation.}
\newblock \emph{Wiley StatsRef: Statistics Reference Online} .

\bibitem[{Malley \emph{et~al.}(2011)Malley, Kruppa, Dasgupta, Malley, \&
  Ziegler}]{randomforestmalley}
\textsc{Malley}, J., J.~\textsc{Kruppa}, A.~\textsc{Dasgupta},
  K.~\textsc{Malley}, \& A.~\textsc{Ziegler} (2011): \enquote{Probability
  machines consistent probability estimation using nonparametric learning
  machines.}
\newblock \emph{Methods of information in medicine} \textbf{51}: pp. 74--81.

\bibitem[{Mucherino \emph{et~al.}(2009)Mucherino, Papajorgji, \&
  Pardalos}]{mucherino2009data}
\textsc{Mucherino}, A., P.~\textsc{Papajorgji}, \& P.~M. \textsc{Pardalos}
  (2009): \emph{Data mining in agriculture}, volume~34.
\newblock Springer Science \& Business Media.

\bibitem[{Newson(2002)}]{newson2002parameters}
\textsc{Newson}, R. (2002): \enquote{Parameters behind “nonparametric”
  statistics: Kendall's tau, somers’ d and median differences.}
\newblock \emph{The Stata Journal} \textbf{2(1)}: pp. 45--64.

\bibitem[{Newson(2014)}]{newson2014interpretation}
\textsc{Newson}, R.~B. (2014): \enquote{Interpretation of somers' d under four
  simple models.}

\bibitem[{Provost \& Fawcett(2013)}]{provost2013data}
\textsc{Provost}, F. \& T.~\textsc{Fawcett} (2013): \emph{Data Science for
  Business: What you need to know about data mining and data-analytic
  thinking}.
\newblock " O'Reilly Media, Inc.".

\bibitem[{Ribeiro \emph{et~al.}(2016)Ribeiro, Singh, \&
  Guestrin}]{ribeiro2016should}
\textsc{Ribeiro}, M.~T., S.~\textsc{Singh}, \& C.~\textsc{Guestrin} (2016):
  \enquote{" why should i trust you?" explaining the predictions of any
  classifier.}
\newblock In \enquote{Proceedings of the 22nd ACM SIGKDD international
  conference on knowledge discovery and data mining,} pp. 1135--1144.

\bibitem[{Wendler \& Gr{\"o}ttrup(2021)}]{wendler2021data}
\textsc{Wendler}, T. \& S.~\textsc{Gr{\"o}ttrup} (2021): \emph{Data Mining with
  SPSS Modeler: Theory, Exercises and Solutions}.
\newblock Cham: Springer.

\bibitem[{Witten \emph{et~al.}(2011)Witten, Frank, Hall, \&
  Pal}]{witten2011data}
\textsc{Witten}, I.~H., E.~\textsc{Frank}, M.~\textsc{Hall}, \& C.~\textsc{Pal}
  (2011): \emph{Data Mining: Practical Machine Learning Tools and Techniques}.
\newblock Morgan Kaufmann, 4th edition.

\end{thebibliography}
