\chapter{Conclusion}
\label{conclusion}

The main contribution of this thesis is to develop a custom machine learning implementation framework using Python which is further applied to the application scoring data set of US home equity loans (HMEQ).
In this thesis, we use 8 machine learning classification models - Logistic Regression, Decision Tree, Gaussian Naive Bayes, K--Nearest Neighbors, Random Forest, Gradient Boosting, Support Vector Machine, and Neural Network (Multi--Layer Perceptron). As evaluiation metrics, we use F1 score, Precision, Recall, Accuracy, Matthews Correlation Coefficient, AUC, Kolmogorov--Smirnov Distance, Somers' D, Brier Score Loss and Log Loss.

Within this thesis, we have covered theoretical background of credit risk as well as particular machine learning algorithms and the evaluation of classification models, including more advanced machine learning techniques such as ADASYN oversmapling, Optimal Binning, Bayesian Optimization or Forward Sequential Feature Selection, as all described in \autoref{chap:two}.
In \autoref{chap:three} we have proposed five hypothesess, including the literature review with the main focus on HMEQ--based studies of Aras \citep{serkan2021bagging} and Zurada \citep{zurada2014classification}.

The \autoref{chap:four} pertains to the very extensive empirical analysis, i.e., machine learning implementation on HMEQ data set.  This includes the high--level overview of the custom machine learning framework, repository and envinronment structure in Python (\autoref{sec:repo}), data exploration, data preprocessing, hyperparameter tuning, feature selection, model selection, model recalibration, model evaluation and final machine learning deployment.
Within data exploration (\autoref{sec:dataexploration}), we delve into the description of the data set, disribution analysis and association analysis.

This chapter further employs an data preprocessing step (\autoref{sec:dataprep}), including the data split into training set (for hyperparameter tuning, feature selection and model training), validation set (for model selection), and test set (for model evaluation).
Such data is split into ratio 70:15:15 with stratification and further, ADASYN oversampling is performed in order to balance the default distribution (\autoref{subsec:data-split-ADASYN}).
The next step of data preprocessing involves Optimal Binning (\autoref{subsec:prep-optbinning}), which is utilized using \lstinline{optbinning} module in Python developed by Navas--Palencia \citep{navas2020optimal}. This module is employed in order to discretize the numeric features into interval bins and categorical features into subgroups of categories, both are optimally binned with respect to the default, and further transforms these bins using Weight--of--Evidence.


\autoref{chap:four} also contains modelling part (\autoref{sec:modelling}), which includes Bayesian Optimization, feature selection and model selection.
Bayesian Optimization is used for hyperparameter tuning, particularly we use \lstinline{scikit-optimize} module which employes Bayesian Optimization with 50 iterations, stratified 10--fold cross validation, while maximizing F1 score. For each model, we further define hyperparameters' spaces over which the model is tuned (\autoref{subsec:hyperoptbayes}).
The feature selection (\autoref{subsec:feature-selection}) is performed using custom algorithm developed by the author, which iterates over the models. Each model is tuned with Bayesian Optimization on training set and is further used as an input estimator within Forward Sequential Feature Selection on training set, which returns subset of optimal features for given model. Since we have eight models, we obtain eight subset of features.
Within the model selection \autoref{subsec:modelselection}, each model is tuned on each subset of selected features on training set, and then is evaluated on the validation set by computing several evaluation metrics.
Instead of using standard classification threshold of 0.5, we compute an optimal threshold using Youden index. Since we have 8 models and 8 subsets of features, we obtain 64 models in total.
All the models are ranked on each metric, based on which we calculate a rank score as a weighted average of the individual ranks, where the weights are set explicitly by the author. Based on the rank score, we determine the final rank. The model with the highest rank (i.e., rank of 1) is then selected as the final model for both evaluation and deployment.
The final model is \textbf{Gradient Boosting} which was trained on the features selected by \textbf{Multi--Layer Perceptron}.

Such final model is then recalibrated on the joined training and validation set and also, the optimal threshold is recalculated using Youden index, as described in \autoref{subsec:modelrecal}.
afterwards, the final recalibrated model is evaluated on test set (\autoref{sec:modeleval}), where we assess the model performance, inspect impact of the recalibration on the model's predictive power and also we delve into black--box model explainability using feature importances and SHAP summary plot. It is evident that debt--to--income ratio and number of delinquent credit lines are the main default drivers.

In the last part of machine learning implementation, the final model is deployed into a production enviroment (\autoref{sec:deployment}).
Prior the deployment, the final model and its optimal threshold are recalibrated on the whole data set (training, validation and test set) in order to maximize the predictive power of the model.
Particularly, such model is deployed as a web application built in Flask (back--end) and HTML with CSS and JavaScript elements (front-end). The web application itself is temporarily deployed on PythonAnywhere cloud platform which is accessible via the folllowing link: \url{http://ml-credit-risk-app-petrngn.pythonanywhere.com/}.
This application requires to fill in the loan application form where its fields correspond to the features on which the final model was trained (\autoref{fig:flaskform}). After a submission of the form, the application processes the inputs, bins them and transforms them into WoE values and afterwards, it predicts the result, whether the loan is rejected or approved based on the optimal threshold, and the probability of default. The result is then displayed on the web page (\autoref{fig:flaskres}).
The application also displays LIME which depicts the local Explainability of the model around the given prediction, i.e., it shows the contribution of each feature to the prediction.

In the last \autoref{chap:five}, we summarize the results of this thesis. Particularly, we assess hypotheses' testing (\autoref{sec:hypotest}) which is depicted in  \autoref{tab:hypoconclusion}.

This chapter also includes the discussion of the model rankings results with the results of the studies of Aras \citep{serkan2021bagging} and Zurada \citep{zurada2014classification} as discussed in \autoref{sec:comparisonfinal}.
While the results of Aras' study are in line with the results of this thesis, i.e., Random Forest and K-Nearest Neighbors perform well whereas Decision Tree, Logistic Regression and Gaussian Naive Bayes perform poorly,
the results of Zurada's study are not in line with the results of this thesis, as its best models are Multi--Layer Perceptron and Decision Tree, which are outperformed by Gradient Boosting, Random Forest, K-Nearest Neighbors and Support Vector Machine.
Further, Zurada ranks K-Nearest Neighbors as one the worst performing models (after Logistic Regression), whereas K-Nearest Neighbors performs well in this thesis and outperforms Support Vector Machine, Multi--Layer Perceptron, Decision Tree, Logistic Regression and Gaussian Naive Bayes.
Since either thesis' author, Zurada and Aras used different data preprocessing steps, hyperparameter tuning approach and spaces, evaluation metrics and software tools, we should refrain from making direct comparisons due to the incomparability of the results, given the factors mentioned above.

Regarding the key findings of this thesis, particularly of the machine learning implementation \autoref{sec:keyfindings}, besides the findings observed within hypotheses' testing and studies' comparisons, we further observe that ADASYN oversampling generates more default--case instances which are managers
which is attributed to the nature of ADASYN oversampling,
which generates more synthetic default--case samples for such default cases which are hard--to--learn by ADASYN,
thus defaulted managers are hard--to--learn by ADASYN (\autoref{subsec:data-split-ADASYN}).
By generating more syntehtic instances which are hard--to--learn, it is expected that the model's performance will be enhanced.
Another finding regards the impact of the missing value of debt--to--income ratio within default prediction, in particular, if the debt--to--income ratio is missing, it increases the probability of default (\autoref{fig:shap}), since bin capturing missing values has negative WoE coefficient (\autoref{fig:woedist}).
Following this observation, also higher of debt-to--income ratio and/or number of delinquent credit lines increase the probability of default as can be seen in SHAP summary plot (\autoref{fig:shap}) and WoE distribution (\autoref{fig:woedist}).
Further, most of the models are conservative since their optimal thresholds are lower than the standard classification threshold 0.5, i.e., they are more likely to reject the loan application than to approve it, as depicted in \autoref{fig:thresdistclean}.
Moreover, ensemble models, such as Gradient Boosting and Random Forest, outperform Support Vector Machine and Multi--Layer Perceptron, whereas the less complex, white--box models, such as Decision Tree, Logistic Regression and Gaussian Naive Bayes have very poor performance, as shown in \autoref{fig:avgrankdist}.
Last but not least, Black--box models which are complex require more features than transparent white box--models to perform well within feature selection (\autoref{fig:fsrec}).

In the final part, we discuss the contribution of this thesis which is further elaborated in detail in \autoref{sec:contributions}, and the recommendations for future research \autoref{sec:keyfindings}, given the limitation of the analyzed data set which might not be representative for the current situation in the financial industry.
We recommend to use more relevant actual data to capture current market situation; use bigger data to increase the training sample size; use macroeconomic data to dynamically capture the current economic state;
use TensorFlow/Keras or PyTorch for development of Neural Network which is more versatile than Scikit-learn; use Django for machine learning deployment which is more suitable for full--stack projects than Flask;
implement machine learning in other components of credit risk modelling, such as EAD, LGD, recovery rates or ECL itself; use Optuna module for hyperparameter tuning which is the state--of--the--art hyperparameter optimization framework; use H2O module for automated machine learning.