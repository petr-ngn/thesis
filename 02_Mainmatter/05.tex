\chapter{Summary of Results}
\label{chap:five}
In this chapter, we assess the hypotheses stated in \autoref{sec:hypo} based on the results derived from machine learning implementation described in \autoref{chap:four}.
Moreover, in response to the outcomes derived from this thesis, we proffer suggestions for subsequent studies. These recommendations, shaped by our findings, serve as a roadmap to guide future investigations within this field.

\section{Hypotheses' Testing}
\noindent \textbf{Result \#1:} \textit{The recalibration of the model \underline{DOES} enhace model performance on HMEQ data set.}

We fail to reject the \textbf{Hypothesis \#1} because all we observe improvements across the metrics within evaluation on test set when the final model was re-trained on the joined training and validation set instead solely on the training set.
In particular, all the score metrics and loss metrics have increased and decreased, respectively, as can be seen in \autoref{tab:recab}. Hereby, we can confirm that the recalibration of the model truly boost the model predictive power on HMEQ data set.

\vspace{0.3cm}

\noindent \textbf{Result \#2:} \textit{Neural Network and KNN model \underline{DO NOT} outperform all the models on HMEQ data set.}

We reject the \textbf{Hypothesis \#2} as the final model, which is the best performing, is the Gradient Boosting model as described in \autoref{tab:finalmodelinfo}.
KNN was also outperformed by the Random Forest and in case of Neural Network, it was outperformed by SVM as well as can be seen in \autoref{fig:avgrankdist}.
If we aggregate the model selection results by looking the highest rank per each model, we can observe that the best KNN model had the 12th highest rank, while Neural Network (MLP) exhibited unsatisfactory performance with the 26th highest rank as can be seen in \autoref{tab:maxranks}.
Therefore, our results are not in with line with the outcomes of respective studies \citep{serkan2021bagging,zurada2014classification} which reported that Neural Network and KNN model outperformed all the models on HMEQ data set, respectively.

\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
        \caption[Max--Aggregated Ranks of Models]{Max--Aggregated Ranks of Models}\label{tab:maxranks}
        \begin{tabular}{r r}
    \toprule
    \textbf{Model} & \textbf{Rank}\\
    \midrule
    \hline
    GB & 1 \\ 
    RF & 6 \\ 
    KNN & 12 \\ 
    SVM & 17 \\ 
    MLP & 26 \\ 
    DT & 35 \\ 
    LR & 39 \\
    GNB & 52 \\
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}

        \centering{\begin{source}Author's results in Python\end{source}}\vspace{-1em}
\end{table}

\vspace{0.3cm}

\noindent \textbf{Result \#3:} \textit{Black--box models \underline{DO} perform better than the white--box models on HMEQ data set.}

We fail to reject the \textbf{Hypothesis \#3} as the black--box models truly outperformed the white--box models as can be seen in \autoref{fig:avgrankdistbbwb} which depicts the distribution of ranks for both black--box and white--box models.
Most of the white--box models were ranked in the bottom half within the model selection, whereas the 10 best performing models consisted out of black--box models only, namely Gradient Boosting and Random Forest.
Even though that some black--box models' performances were weak and other white--box models exhibited relatively high ranks, we can still conclude that the black--box models outperformed the white--box models on average as the median of the black--box models' ranks is lower than the white--box models' median.
\vspace{0.3cm}

\noindent \textbf{Result \#4:} \textit{The longer execution time of a model \underline{DOES NOT} indicate better performance on HMEQ data set.}

We reject the \textbf{Hypothesis \#4} as according to \autoref{fig:timedist}, even though the Neural Network (MLP) model had the longest execution time on average, it underperformed several models such as Gradient Boosting, Random Forest, KNN and SVM which took significantly less time to execute. The same can be observed from \autoref{fig:scattertime}, where Neural Network performed poorly regardless the length of the execution time.
\vspace{0.3cm}

\noindent \textbf{Result \#5:} \textit{Debt and delinquency features \underline{ARE} the main default drivers on HMEQ data set.}

We fail to reject the \textbf{Hypothesis \#5} according to \autoref{fig:fi} which depicts the feature importance of the final model (Gradient Boosting). As can be seen, the most important features are debt--to--income ratio \texttt{DEBTINC} and number of delinquent credit lines \texttt{DELINQ}.
Based on the SHAP values depicted in \autoref{fig:shap}, we can observe that the negative values of texttt{DEBTINC} and \texttt{DELINQ} positively contribute to the model's predictions of the target variable, whereas the positive values negatively contribute to the model's predictions.
In other words, the higher value of such features, the higher probability of default and vice versa.
Since the features' values are encoded as WoE, the negative WoE values indicates larger distribution of defaulters compared to non--defaulters in given bins.
Based on the WoE bins distribution in \autoref{fig:woedist}, we can observe negative WoE values for bins where the debt--to--income ratio is extremely high or is missing. Regarding to the number of delinquent credit lines, we can observe a negative WoE value for bin corresponding to the relatively high number of delinquent credit lines.
Therefore, if the loan applicant has either high or missing debt--to--income ratio and/or relatively high number of delinquent credit lines,  he would be likely to default.

\section{Comparison with other HMEQ Studies}

Within the literature review in \autoref{chap:three}, we explored the studies of Aras \citep{serkan2021bagging} and Zurada \citep{zurada2014classification} which analyzed the HMEQ data set, i.e., the same data set as in this thesis.
In this section, we contrast their results with those of this thesis. Particularly, we compare the models' rankings from \autoref{tab:serkanresultsranks} and \autoref{tab:zuradaresultsranks}, respectively, with the author's rankings derived from \autoref{tab:maxranks}, which is summarized in the following \autoref{tab:comparisonfinal}.
Upon examination, it is apparent that none of the studies considered the Gradient Boosting model, which happens to be the best-performing model in this thesis.

With respect to the study of Aras \citep{serkan2021bagging}, when excluding our Gradient Boosting model, we can agree that Random Forest and KNN are the top-performing models. However, there is a discrepancy in the order of their performance. Aras found that KNN outperformed Random Forest, whereas in this thesis, Random Forest outperformed KNN.
Furthermore, it is worth noting that the worst-performing models, namely Decision Tree, Logistic Regression, and Gaussian Naive Bayes, align with both this thesis and Aras' study. Specifically, Aras identified Logistic Regression as the worst-performing model, while in this thesis, Gaussian Naive Bayes exhibited the poorest performance.

Regarding the Zurada's study \citep{zurada2014classification}, we again concur that Logistic Regression is one of the worst performing models.
However, there are significant disparities in the rankings of other models compared to the results obtained in this thesis.
It is evident from the findings that MLP was identified as the best performing model in Zurada's study, whereas in this thesis, MLP was ranked as the 5th best performing model, placing it in the second half of the ranked models.
In contrast, while KNN was ranked as the 3rd best performing model in this thesis, it received the position of the 2nd worst performing model according to Zurada.
Conversely, Zurada ranked Decision Tree as the best second performing model, while in this thesis, it was positioned as the 6th best performing model.
However, both Zurada's and Aras' studies, consistent with this thesis, ranked the SVM model in the middle range, aligning with our findings.
\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
        \caption[Ranking Results Comparison based on HMEQ Data Set]{Ranking Results Comparison based on HMEQ Data Set}\label{tab:comparisonfinal}
        \begin{tabular}{r r r r}
    \toprule
    \textbf{Model} & \textbf{This Thesis} & \textbf{\citep{serkan2021bagging}} & \textbf{\citep{zurada2014classification}}\\
    \midrule
    \hline
    GB & 1 & - & - \\ 
    RF & 2 & 2 & - \\ 
    KNN & 3 & 1 & 4 \\ 
    SVM & 4 & 3 & 3 \\ 
    MLP & 5 & - & 1 \\ 
    DT & 6 & 4 & 2 \\ 
    LR & 7 & 6 & 5 \\
    GNB & 8 & 5 & - \\
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}

        \centering{\begin{source}Author's results in Python, Rankings of \citep{serkan2021bagging}, \citep{zurada2014classification}\end{source}}\vspace{-1em}
\end{table}

It is important to note that the author of the thesis, Aras, and Zurada employed distinct data preprocessing techniques and employed different data splits, leading to variations in the training, tuning, and evaluation samples, resulting in disparate outcomes.
While Zurada and Aras utilized Grid Search with cross-validation for hyperparameter tuning, the author of the thesis employed Bayesian Optimization with stratified cross-validation and different hyperparameter spaces.
Additionally, each author utilized different evaluation metrics. Aras utilized Accuracy, Recall, Precision, F1, and Matthews Correlation Coefficient, while Zurada employed Accuracy, Recall, and AUC.
In contrast, the author of the thesis employed a broader range of metrics, including Accuracy, Recall, Precision, F1, Matthews Correlation Coefficient, AUC, Kolmogorov-Smirnov Distance, Somers' D, Brier Score Loss, and Log Loss.
Moreover, it is worth mentioning that the authors may have used different software tools. The thesis' author utilized Python, while Zurada employed Weka, and the software used by Aras is unclear.
These various factors contribute to the discrepancies in the obtained results among the studies and the thesis. Therefore, it is advisable to refrain from making direct comparisons due to the incomparability of the results.
\vspace{0.3cm}

\section{Key Findings}

\section{Contributions}

\section{Recommendations}
Despite the complexity of the custom machine learning solution applied in this thesis, and the hundreds of hours invested by the author, it still has its limitations and drawbacks. We aim to address these issues in this section and provide suggestions for future studies.
\begin{enumerate}\setlength\itemsep{0em}
    \item \textbf{Use more relevant and actual data} - Given the nature of the data set analyzed in this thesis, it might not be the most relevant for the current market situation. Thus, using an up-to-date data would either lead to a more realistic generalization \citep {kumar2021blockchain} or an improvement in model's performance \citep{karatas2020increasing}.
    \item \textbf{Use bigger data} - Since the real bank data sets often contain hunders of thousands or even millions of instances, and the number of features can be in hundreds or even higher, it would be beneficial to use bigger data sets to train the models on. Therefore, having bigger sample size for the training, we can enhace model's performance \citep{ng2020influence}, as the model would be able to learn more complex patterns in the data. 
    \item \textbf{Use behavioral scoring data} - The data set used in this thesis is based on the application data only, however, the behavioral scoring data is often used in the real--world applications as well. Therefore, it would be also beneficial to use behavioral scoring data which would dynamically capture the client's information about his behavior and it would help the bank to make more informed decision about how to deal with the existing clients \citep{li2012overview}.
    \item \textbf{Use macroeconomic data} - Using macroeconomic data such as GDP, unemployment rate, inflation rate, interest rate etc. would allow to create macroeceonomic forecasts, i.e., to take into account possible changes in the macroeconomic changes by incorporating forward--looking information \citep{jakubik2007macroeconomic}.
    Thanks to that, one would be able to achieve more more reliable estimates by would dynamically capturing the current economic situation.
    \item \textbf{Use TensorFlow/Keras or PyTorch for NN development} - In this thesis, we used Neural Network (Multi--Layer Percepton) from \lstinline{Scikit-learn} module. Howeover, in ML engineering or deep learning, the Neural Networks are mostly developed using TensorFlow/Keras or Keras modules which are more efficient and provide more flexibility \citep{gevorkyan2019review}.
    \item \textbf{Django for ML deployment} - In this thesis, a Flask framework was used for ML deployment as a web application. Although Flask is a great tool for ML deployment when it comes to the small projects or micro-framework solutions, it is not suitable for production environment.
    Therefore, we recommend to use Django framework for ML deployment, which is a full--stack Python--based web application framework and is more suitable for production environment and creating larger more complex database--backed websites and applications \citep{khatri2023}.
    \item \textbf{ML implementation in other credit risk modelling components} - Besides applying ML prediction models in credit risk for default prediction (i.e., PD), it would be also beneficial to apply ML models in other credit risk modelling components such as LGD and EAD, ECL \citep{munkhdalai2019empirical, grzybowska2020application} or even for macroeceonomic forecasting \citep{hall2018machine}.
    \item \textbf{Hyperparameter Optimization with Optuna} - In this thesis, we used Bayesian Optimization from \lstinline{Scikit-optimize} module for hyperparameter optimization. However, there are more efficient framwork, namely Optuna module, which is deemed as the next--generation hyperparameter optimization software which allows to construct the hyperparameter search space dynamically, efficient implementation of searching and pruning strategies and it also provides
    a versatile architecture which can be deployed in scalable distributed computing or light--weight experiments conducted through interactive interface \citep{akiba2019optuna}.
    \item \textbf{H2O ML library} - H2O is an automated machine learning library which is more efficient than \lstinline{Scikit-learn} and provides more flexibility in terms of faster scoring capabilities or producing high quality models suitable for deployment in enterprise envinronment \citep{ledell2020h2o}. 
    According to H2O's documentation, it provides wrapper functions which perform a large number of modelling-related tasks that would typically require many lines of code and it can be also used for automating the machine learning workflow including automatic training and tuning of many models \citep{H2Oai2023}.
\end{enumerate}