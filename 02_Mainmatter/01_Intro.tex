\chapter{Introduction}
\label{chap:one}


Machine learning is becoming increasingly important in the financial industry as it can bring competitive edge. Many financial institutions have already utilized the machine learning techniques for assessing creditworthiness of the clients and predicting their future behavior \citep{PwC2023}. \newline
As the backbone of most financial institutions, credit risk modelling plays an integral role in ensuring the stability and profitability of these establishments as it primarily revolves around assessing the likelihood that a borrower will default on a debt by failing to make required payments.

Conventional methodologies for credit risk assessment have relied on statistical techniques and rule-based systems, which have been known to deliver sound results.
However, with the exponential growth of data in recent years, financial institutions are confronted with the challenge and opportunity to extract valuable insights from this information deluge.
In an era where data-driven decision making is becoming the norm rather than the exception, the application of machine learning in credit risk modelling is not just an interesting research area, but also a future practical necessity.

In this thesis, the main goal is to implement a custom machine learning framework developed in Python, which includes data exploration, data preprocessing, Bayesian hyperparameter optimization, feature selection, model selection, model recalibration, and model evaluation, and further to develop a custom web application, within which the trained model is deployed, into a production environment, using Flask and HTML with CSS and JavaScript elements.
Such comprehensive machine learning framework is applied on an exemplary data set of US home equity loans (HMEQ) acquired from Credit Analytics \citep{baesens2016credit}.
Regardless of the potential discrepancy between such exemplary data set and the current state of the loan market, the objective of this thesis is rather to showcase a versatile machine learning implementation framework that can be effectively employed for diverse data sets, transcending industry boundaries and fulfilling varying analytical requirements.
In this thesis, we employ eight different machine learning algorithms, namely Logistic Regression, Decision Tree, Gaussian Naive Bayes, K-Nearest Neighbors, Random Forest, Support Vector Machine, Gradient Boosting, and Neural Network.
As evaluation metrics in order to assess model's performance, we use F1 score, Precision, Recall, Accuracy, Matthews Correlation Coefficient, Area Under the (ROC) Curve (henceforth AUC), Kolmogorov--Smirnov Distance, Somers' D, Brier Score Loss and Log Loss.

This thesis is divided into five chapters (2-5).  \autoref{chap:two} regards the theoretical background which introduces the credit risk modelling and machine learning concepts. Particularly, credit risk definition, regulation and credit scoring approaches are presented.
Furthermore, machine learning terminology, machine learning algorithms and evaluation metrics are described.
Morevover, more advanced machine learning techniques are introduced, such as ADASYN oversampling, Optimal Binning, Bayesian hyperparameter optimization or Forward Sequential Feature Selection.

\autoref{chap:three} assesses literature review from both credit risk modelling and machine learning fields. Moreover, we propose five hypotheses either based on HMEQ--based studies \citep{serkan2021bagging, zurada2014classification}, or based on researches related machine learning application in different sectors \citep{de2023predicting, pintelas2020grey, wu2018accurate}.

In \autoref{chap:four}, we present very extensive machine learning implementation framework, including a description of repository and environment structure of given implementation. Within the data exploration, we describe the analyzed data, inspect distributions of variables and also perform statistical association analysis in order to infer some relationships between the default and the predictors and between the predictors themselves.
In the data preprocessing, we perform split of data into training set (for model building, hyperparameter tuning and feature selection), validation set (for model selection) and test set (for model evaluation).
Further we perform ADASYN oversampling to balanced skewed defeault distribution and employ Optimal Binning which bins numerical predictors into interval bins and categorical predictors into categorical group bins, optimally with respect to the default status. Such bins are then transformed using Weight--of--Evidence.

In the modelling part, we assess Bayesian Optimization for hyperparameter tuning and propose two custom algorithms. Feature selection algorithm which takes each model, tunes it with Bayesian Optimization and performs Forward Sequential Feature Selection, which returns a subset of optimal predictors.
Further, model selection algorithm which takes each model, tunes it with Bayesian Optimization on each subset of optimal predictors, and then evaluates it on the validation set using the proposed range of evaluation metrics. Instead of using standard classification threshold of 0.5, we compute an optimal threshold using Youden index.
Additionally, each model is then ranked accordingly to each evaluation metric and afterwards, a rank score is calculated for each model as a weighted average of individual ranks, where the weights are explicitly defined by the author. The model with the highest rank score is then selected as the best model.

Within the model recalibration part, the final model retrained on the joined training set and validation set, as well as the optimal threshold is recalculated in order to ehnace model's performance.
In the evaluation section, the model is evaluated on the test set. Particularly, we assess model's performance by constructing confusion matrix or ROC curve, as well as by computing evaluation metrics mentioned above and comparing them with the results of the non-recalibrated model in order to assess impact of recalibration. Further, model explainability is inspected using feature importances and SHAP summary plot for global explainability of the model.

In the final part of machine learning implementation, we deploy the final model into a production environment as a web application using Flask and HTML with CSS and JavaScript elements. Prior the deployment, the final model and the optimal threshold are recalibrated on the joined training set, validation set and test set in order to enhace model's ability to generalize. 
The web application is temporarily deployed on PythonAnywhere cloud platform and it requires to fill in a loan application form. Once the form is submitted, the application processes the inputs and returns a result whether the loan would be approved or not according to the final model, as well as the probability of default. Further, this application also depicts LIME, i.e., local explainability of the model around given prediction, which how each predictor contributes to the prediction.

In the final \autoref{chap:five}, we summarize the results of this thesis. In particular, we assess the hypotheses proposed in \autoref{chap:three}, discuss the results of the machine learning implementation in this thesis with the results of Aras \citep{serkan2021bagging} and Zurada \citep{zurada2014classification}, present the key findings of machine learning implementation, outline the author's contribution in this thesis and lastly, propose some future research directions.

