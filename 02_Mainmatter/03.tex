\chapter{Literature Review}
\label{chap:three}

In this thesis, our objective is to explore the application and implementation of machine learning in credit scoring. We will examine relevant studies in two areas: machine learning applications specifically related to credit scoring and machine learning implementations in various other domains. The aim is to identify machine learning techniques from non-credit scoring contexts that could potentially be useful in credit scoring. We will evaluate the impact of these techniques through hypothesis testing.

Regarding studies related to machine learning in credit scoring, we will analyze not only the research focused on the data set used in this thesis, but also studies that involve different data sets. This comprehensive approach is intended to develop a broader understanding and facilitate the discovery of valuable insights and methodologies that can be applied across various data sets in the credit scoring field.

It is a known fact that the distribution of the default status is imbalanced in the credit scoring domain, since the non--default cases are overpresented compared to the default cases. Several studies have already address such issue.
For instance, Owusu and others \citep{owusu2023deep} employed ADASYN oversampling on the peer-to-peer loans data from American Lending Club, which exhibited an improvement in the model's performance in comparison to other benchmark studies.
Another way to address such imbalance issue is changing the classication cut--off point.
By default, the cut--off point is set to 0.5, which means that if the predicted probability score exceeds the threshold, it will be classified as default and vice versa. Kazemi and others \citep{kazemi2023estimation} demonstrated that utilization of the customized cut--off points leads to a more accurate classification compared to the default threshold value of 0.5, based on German and Australian credit data sets available to the public in the UCI machine learning data repository.


Whether to choose transparent, white--box model such as logistic regression or a complex, black--box model such as ensemble models or neural network depends on various factors, such as type of the data set, sample size, data preprocessing techniques, hyperparameter tuning, or even a software used to implement the model.
While Tepl\'{y} and Polena \citep{teply2020best} suggest that Logistic Regression is the best performing model on peer-to-peer loans data from Lending Club, Aniceto and others \citep{aniceto2020machine} demonstrate that the ensemble models, namely Random Forest and AdaBoost, outperformed the Logistic Regression on a large Brazilian bank's loan data set.


In this thesis, a data set of US home equity loans (HMEQ) is analyzed, which is further described in \autoref{subsec:datadescript}.
Regarding the studies pertaining to the HMEQ data set, the study of Aras \citep{serkan2021bagging} and the study of Zurada \citep{zurada2014classification} are deemed to be the most relevant to this thesis.


With respect to the former study, the author performed imputation of missing values with mode and mean, respectively, and for an evalutation, author used test size of size 596 instances.
Author also performed an oversampling on the training set using Random Oversampling in order to balance the default status distribution.
Regarding the fitted models which are also used in this thesis, author used KNN, Random Forest (RF), SVM, Decision Tree (DT), Gaussian Naive Bayes (GNB) and Logistic Regression (LR).
Such models were also tuned using Grid Search method with 10--fold cross validation.

Within the evalutation, author assessed Accuracy (Acc.), Recall (Rec.), Precision (Prec.), F1 and Matthews Correlation Coefficient (MCC) metrics. The particular results are summarized in \autoref{tab:serkanresults}.
Since the default distribution is imbalanced on the test set, the most relevant metrics are F1 and MCC.
As can be seen, only three models exceeded the 0.8 and 0.75 threshold for F1 and MCC, respectively, namely KNN, RF and SVM. Whereas Gaussian Naive Bayes and Logistic Regression performed poorly as they did not even exceeded 0.5 threshold for F1 and MCC, respectively.
However, it is not appropriate to compare the computed the metrics with the results of this thesis, since the test set size is not the same, and the data preprocessing and modelling is different as well.
\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Evaluation Results \citep{serkan2021bagging}]{Evaluation Results \citep{serkan2021bagging}}\label{tab:serkanresults}
    \begin{tabular}{r r r r r r}
    \toprule
    \textbf{Model} & \textbf{Acc.} & \textbf{Rec.} & \textbf{Prec.} & \textbf{F1} & \textbf{MCC} \\
    \midrule
    \hline
	
	KNN & 0.953 & 0.789 & 0.980 & 0.874 & 0.853 \\
    RF & 0.930 & 0.789 & 0.858 & 0.822 & 0.779 \\
    SVM & 0.926 & 0.756 & 0.869 & 0.809 & 0.766 \\
    DT & 0.898 & 0.683 & 0.792 & 0.734 & 0.674 \\
    GNB & 0.795 & 0.480 & 0.504 & 0.492 & 0.364 \\
	LR & 0.705 & 0.691 & 0.381 & 0.491 & 0.334 \\
	
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}\citep{serkan2021bagging}\end{source}}\vspace{-1em}
\end{table}

If we rank the models by each computed metric descendingly,we can explicitly derive a rank score as an average of the ranks. Based on the rank scores, we can then rank the models, as shown in \autoref{tab:serkanresultsranks}. As can be seen, KNN dominantly outperformed all the models across all the metrics, while the black--box models such as Random Forest and SVM performed also performed well but not as well as KNN.
On the other hand, Gaussian Naive Bayes and Logistic Regression performance exhibited very unsatisfactory performance.
Therefore, according to Aras, one would expect that KNN and the black--box models (Random Forest and/or SVM) would perform well on the HMEQ data set.

\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Evaluation Results - Ranked \citep{serkan2021bagging}]{Evaluation Results - Ranked \citep{serkan2021bagging}}\label{tab:serkanresultsranks}
    \begin{tabular}{r r r r r r r r}
    \toprule
    \textbf{Model} & \textbf{Acc.} & \textbf{Rec.} & \textbf{Prec.} & \textbf{F1} & \textbf{MCC} & \textbf{Score} & \textbf{Rank} \\
    \midrule
    \hline
	
    KNN & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\ 
    RF & 2 & 1 & 3 & 2 & 2 & 2 & 2 \\ 
    SVM & 3 & 2 & 2 & 3 & 3 & 2.6 & 3 \\ 
    DT & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\ 
    GNB & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\ 
    LR & 6 & 3 & 6 & 6 & 6 & 5.4 & 6 \\ 
	
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}Ranking of \citep{serkan2021bagging}\end{source}}\vspace{-1em}
\end{table}


The latter study pertains to the reasearch conducted by Zurada and others \citep{zurada2014classification}. Likewise Aras, they also analyzed various classification models, but only relevant ones to this thesis are further discussed.
Namely, the authors trained Neural Network (NN), Decision Tree (DT), Support Vector Machine (SVM), K--Nearest Neighbors (KNN) and Logistic Regression (LR), using Weka software.
Authors did not mention which imputation technique did they use, nor the data split ratio (i.e., the test size used for an evulation), but likwise Aras, they conducted a hyperparameter tuning using Grid Search with 10--fold cross validation.

The evalulation results are depicted in \autoref{tab:zuradaresults}, namely the computed metrics such as Accuracy (Acc.), Recall (Rec.) and AUC.
As can be seen, KNN model, which was the best one in case of Zurada's study, yielded a very deteriorated performance, while Logistic regression still performed badly.
On the other, Neural Network was the best performing model as a black--box model. Surprisingly, also Decision Tree showcased relatively high performance in contrast to Zurada's study, where Decision Tree's performance was quite weak.
\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Evaluation Results \citep{zurada2014classification}]{Evaluation Results \citep{zurada2014classification}}\label{tab:zuradaresults}
    \begin{tabular}{r r r r}
    \toprule
    \textbf{Model} & \textbf{Acc.} & \textbf{Rec.} & \textbf{AUC}\\
    \midrule
    \hline
    NN & 0.869 & 0.590 & 0.863 \\
    DT & 0.889 & 0.548 & 0.844 \\
    SVM & 0.848 & 0.346 & 0.810 \\
    KNN & 0.791 & 0.334 & 0.826 \\
    LR & 0.836 & 0.304 & 0.794 \\
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}\citep{zurada2014classification}\end{source}}\vspace{-1em}
\end{table}

The same insights can be derived from ranking the results in the same way as in Zurada's study, as shown in \autoref{tab:zuradaresultsranks}.
Hence, one would expect that Neural Network would perform well on the HMEQ data set, while Logistic Regression's performance would be unsatisfactory.
\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Evaluation Results - Ranked \citep{zurada2014classification}]{Evaluation Results - Ranked \citep{zurada2014classification}}\label{tab:zuradaresultsranks}
    \begin{tabular}{r r r r r r}
    \toprule
    \textbf{Model} & \textbf{Acc.} & \textbf{Rec.} & \textbf{AUC} & \textbf{Score} & \textbf{Rank} \\
    \midrule
    \hline
    NN & 2 & 1 & 1 & 1.33 & 1 \\ 
    DT & 1 & 2 & 2 & 1.67 & 2 \\ 
    SVM & 3 & 3 & 4 & 3.33 & 3 \\ 
    KNN & 5 & 4 & 3 & 4.00 & 4 \\ 
    LR & 4 & 5 & 5 & 4.67 & 5 \\ 
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}Ranking of \citep{zurada2014classification}\end{source}}\vspace{-1em}
\end{table}

Although, both Zurada and Aras ranked the Logistic Regression as the worst performing model and SVM was ranked according to them as the third best model, they did not agree on the ranking of Decision Tree and KNN models.
Such a disagreement can be attributed to the fact they both used different data preprocessing techniques, as well as different hyperparameters' spaces for fine tuning the models and probably also different softwares, random seed, and data split.
We also need to account for a relatively small sample size of the HMEQ data set, which can lead to a high variance in the results.
Therefore, several steps and procedures are applied in this thesis in order to ensure the reliable and robust results and estimates, which are further described in \autoref{sec:modelling}.

\section{Hypotheses}
\label{sec:hypo}

In this section, we formulate the hypotheses, which are partially derived from the literature review disscused above and are further tested in \autoref{sec:hypotest}, i.e., rejected or not rejected, based on the empirical analysis regarding the machine learning implementation in \autoref{chap:four}.
\vspace{0.5cm}

\noindent \textbf{Hypothesis \#1:} \textit{The recalibration of the model enhaces model performance on HMEQ data set.}

The study of de Hond and others \citep{de2023predicting} focuses on the model re-training (recalibration) in order to improve the model performance, based on the health record data from Leiden University Medical Center and Amsterdam University Medical Center for a prediction of readmissions or deaths after ICU discharge.
Such effect is being achieved by either re-training the model on the and newest additional data, hence by increasing the sample size within the model training, we expect the model's predictive power to be enhanced. Such approach is further discussed in \autoref{subsec:modelrecal} and assessed in \autoref{subsec:modelperformance}.
\vspace{0.5cm}


\noindent \textbf{Hypothesis \#2:} \textit{Either Neural Network or KNN model outperforms all the models on HMEQ data set.}

Based on the studies of Aras \citep{serkan2021bagging} and Zurada and others \citep{zurada2014classification}, the highest ranked models in terms of performance were KNN and Neural Network, respectively. These two models outperformed the traditional Logistic Regression, and in case of Aras, KNN also outperformed Decision Tree, Gaussian Naive Bayes, SVM and even Random Forest.
Therefore, we expect that at least one of those two models will outperform all the other models on the HMEQ data set.

\vspace{0.5cm}

\noindent \textbf{Hypothesis \#3:} \textit{Black--box models perform better than the white--box models on HMEQ data set.}

Following the \autoref{sec:mlterms}, there is a trade-off between the model performance and the interpretability, where the black--box models are more complex and less interpretable, but also more accurate, while the white--box models are less complex and more interpretable, but also less accurate \citep{pintelas2020grey}.
With respect to the algorithms described in \autoref{sec:algorithms}, we deem Logistic Regression as a white--box model, since it is interpretable thanks to the estimated coefficients which can indicate the strength and direction impact of each feature on the target variable adjusting for all other features \citep{park2013introduction}.
Another white--box model is a Guassian Naive Bayes as it is based on the Bayes theorem and Gaussian distribution, thus only prior probabilities and conditional means and variances need to be computed per each feature according to \autoref{eq:lastnbmax1} and \autoref{eq:lastgnb}.
Decision Tree can be considered as a white--box model, assuming that that the decision tree is small \citep{goethals2022non}, i.e., if the tree is not too deep and wide, which enhace the comprehensible interpretation of the model.

One would say that KNN is a white--box model, since it just looks for $k$ closest neighbours around given data point. However, the interpretation of the model might not be straightforward if the $k$ is large and/or when having higher number of features which increases the dimensionality, as the model might be too complex.
According to Loyala \citep{loyola2019black}, KNN is considered as a black--box model since it is highly dependent on the distance function which might bias the classification results and change in a distance function can lead to different outcomes. Therefore, we consider KNN as a black--box model.
Regarding the rest of the models, they are deemed as black--box models due to the following reasonings:
\begin{itemize}\setlength\itemsep{0em}
    \item \textbf{SVM} - Very complex mathematical operations involved in such algorithm make it difficult to interpret the model \citep{loyola2019black}.
    \item \textbf{NN} - The more layers and units within each layer does the neural network have, the less transparent and comprehensible the model is \citep{bathaee2017artificial}.
    \item \textbf{RF, GBM} - Such ensemble models consist out of multiple decision trees, which are combined together to form a more complex model. Therefore, the interpretation of such models is not straightforward.
\end{itemize}
Therefore, we expect that the black--box models will outperform the white--box models on the HMEQ data set in overall.

\vspace{0.5cm}


\noindent \textbf{Hypothesis \#4:} \textit{The longer execution time of a model, the better performance on HMEQ data set.}

Although, the research paper's objective of Wu and others \citep{wu2018accurate} does not directly regard the relationship between the training (execution) time and the model performance, but rather the accuracy of indoor localization techniques in two different environments.
Howeover, such study also inspects the evaluation of Bayes Net, SVM and Random Forest models, including their execution time. As shown in \autoref{tab:wuexec}, we can observe that there is a trade-off between model performance and the execution time, as the SVM has the highest accuracy and moderate execution time, while the Bayes Net has the lowest accuracy, but also the shortest execution time.
Therefore, we expect a positive association between the execution time and the model performance.

\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Accuracy vs. Execution Time \citep{wu2018accurate}]{Accuracy vs. Execution Time \citep{wu2018accurate}}\label{tab:wuexec}
    \begin{tabular}{r r r}
    \toprule
    \textbf{Model} & \textbf{Acc.} & \textbf{Time (seconds)} \\
    \midrule
    \hline
    Bayes Net & 0.8521 / 0.8474 & 0.56 / 1.15 \\
    SVM & 0.9328 / 0.9590 & 1.79 / 3.21 \\
    RF & 0.9070 / 0.9574 & 4.92 / 8.84 \\

    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}\citep{wu2018accurate}\end{source}}\vspace{-1em}
\end{table}

\vspace{0.5cm}

\noindent \textbf{Hypothesis \#5:} \textit{The main default drivers are the debt and/or delinquency features on HMEQ data set.}

In the study of Zurada and others \citep{zurada2014classification}, the authors also inspected the significance of features in order to reduce the dimensionality. In particular, the authors employed both correlation analysis with the target variable as well as ranking according to information gain.
In this case, the most significant features are debt--to--income ratio, number of derogatory public records and number of delinquent credit lines.
Thus, these features are expected to be the most important indicators when predicting a default.