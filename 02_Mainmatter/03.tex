\chapter{Literature Review}
\label{chap:three}

In this thesis, we examine the studies pertaining to machine learning applications in credit scoring, as well as studies addressing the wide range of machine learning implementations outside the credit scoring domain. The intent is to highlight certain machine learning techniques from non-credit scoring contexts that may be applicable to credit scoring and inspect whether they have positive or negative impact which will be further assessed within hypotheses testing.
In the context of the studies related to machine learning in credit scoring, we inspect not only the studies focusing on the data set analyzed in this thesis, but we aksi  also about the studies encompassing analyses of variety of distinct data sets.
This inclusive approach is emplopyed to foster a more comprehensive and holistic understanding of the subject and to enable the identification of potentially valuable insights and methodologies that can be applied across different data sets within the credit scoring field.

It is a known fact that the distribution of the default status is imbalanced in the credit scoring domain, since the non--default cases are overpresented compared to the default cases. Several studies have already address such issue.
For instance, Owusu and others \citep{owusu2023deep} employed ADASYN oversampling on the peer-to-peer loans data from American Lending Club, which exhibited an improvement in the model's performance in comparison to other benchmark studies.
Another way to address such imbalance issue is changing the classication cut--off point.
By default, the cut--off point is set to 0.5, which means that the predicted probability of default is greater than 0.5. Kazemi and others \citep{kazemi2023estimation} demonstrated that utilization the customized cut--off points leads to a more accurate classification compared to the default threshold value of 0.5, based on German and Australian credit data sets available to the public in the UCI machine learning data repository.


Therefore, the studies which address the imbalanced data sets are also included in this thesis. The imbalanced data sets are also addressed in the context of the studies which analyze the HMEQ data set, since the default status distribution is also imbalanced in this data set.

Petr Teply, Michal Polena - Best classification algorithms in peer-to-peer lending


Another research of Aras \citep{serkan2021bagging} also studies the HMEQ data set.
Particularly, the author performed imputation of missing values with mode and mean, respectively and for an evalutation, author used test size of size 596 instances.
Author also performed an oversampling on the training set using Random Oversampling in order to balanced the default status distribution.
Regarding the fitted models which are also used in this thesis, the author used KNN, Random Forest (RF), SVM, Decision Tree (DT), Gaussian Naive Bayes (GNB) and Logistic Regression (LR).
Such models were also tuned using Grid Search method with 10--fold cross validation.

Within the evalutation, author used Accuracy (Acc.), Recall (Rec.), Precision (Prec.), F1 and Matthews Correlation Coefficient (MCC) metrics. The particular results are summarized in \autoref{tab:serkanresults}.
Since the default distribution is imbalanced on the test set, the most relevant metrics are F1 and MCC.
As can be seen, only three models exceeded the 0.8 and 0.75 threshold for F1 and MCC, respectively, namely KNN, RF and SVM. Whereas Gaussian Naive Bayes and Logistic Regression performed poorly as they did not even exceeded 0.5 threshold for F1 and MCC, respectively.
However, it is not appropriate to compare the computed the metrics with the results of this thesis, since the test set size is different, did not performed feature selection and probably used a default classification threshold of 0.5.
\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Serkan Results]{Serkan Results}\label{tab:serkanresults}
    \begin{tabular}{r r r r r r}
    \toprule
    Model & Acc. & Rec. & Prec. & F1 & MCC \\
    \midrule
    \hline
	
	KNN & 0.953 & 0.789 & 0.980 & 0.874 & 0.853 \\
    RF & 0.930 & 0.789 & 0.858 & 0.822 & 0.779 \\
    SVM & 0.926 & 0.756 & 0.869 & 0.809 & 0.766 \\
    DT & 0.898 & 0.683 & 0.792 & 0.734 & 0.674 \\
    GNB & 0.795 & 0.480 & 0.504 & 0.492 & 0.364 \\
	LR & 0.705 & 0.691 & 0.381 & 0.491 & 0.334 \\
	
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}\citep{serkan2021bagging}\end{source}}\vspace{-1em}
\end{table}

If we rank the models by each computed metric descendingly,we can explicitly derive a rank score as an average of the ranks. Based on the rank scores, we can then rank the models, as shown in \autoref{tab:serkanresultsranks}. As can be seen, KNN dominantly outperformed all the models across all the metrics, while the black--box models such as Random Forest and SVM performed also performed well but not as well as KNN.
On the other hand, Gaussian Naive Bayes and Logistic Regression performance exhibited very unsatisfactory performance.
Therefore, according to Aras, it is expected that KNN and the black--box models (Random Forest and/or SVM) would perform well on the HMEQ data set.

\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Serkan Results - Ranked]{Serkan Results - Ranked}\label{tab:serkanresultsranks}
    \begin{tabular}{r r r r r r r r}
    \toprule
    Model & Acc. & Rec. & Prec. & F1 & MCC & Score & Rank \\
    \midrule
    \hline
	
    KNN & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\ 
    RF & 2 & 1 & 3 & 2 & 2 & 2 & 2 \\ 
    SVM & 3 & 2 & 2 & 3 & 3 & 2.6 & 3 \\ 
    DT & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\ 
    GNB & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\ 
    LR & 6 & 3 & 6 & 6 & 6 & 5.4 & 6 \\ 
	
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}Ranking of \citep{serkan2021bagging}\end{source}}\vspace{-1em}
\end{table}


Another study which also analyzes HMEQ data set is reasearch coduncted by Zurada and others \citep{zurada2014classification}. Likewise Aras, they also analyzed various classification models, but only relevant ones to this thesis are further discussed.
Namely, the authors trained Neural Network (NN), Decision Tree (DT), Support Vector Machine (SVM), K--Nearest Neighbors (KNN) and Logistic Regression (LR), using Weka software.
Authors did not mention which imputation technique did they use, nor the data split ratio (i.e., the test size used for an evulation), but likwise Aras, they conducted a hyperparameter tuning using Grid Search with 10--fold cross validation.

The evalulation results are depicted in \autoref{tab:zuradaresults}, namely the computed metrics such as Accuracy (Acc.), Recall (Rec.) and AUC.
As can be seen, KNN model which was the best one in case of Zurada's study, yielded a very deteriorated performance, while Logistic regression still performed badly.
On the other hand the Neural Network was the best performing model as a black--box model. Surprisingly, also Decision Tree showcased relatively high performance  in contrast to Zurada's study, where Decision Tree's performace was weak.
\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Zurada Results]{Zurada Results}\label{tab:zuradaresults}
    \begin{tabular}{r r r r}
    \toprule
    Model & Acc. & Rec. & AUC\\
    \midrule
    \hline
    NN & 0.869 & 0.590 & 0.863 \\
    DT & 0.889 & 0.548 & 0.844 \\
    SVM & 0.848 & 0.346 & 0.810 \\
    KNN & 0.791 & 0.334 & 0.826 \\
    LR & 0.836 & 0.304 & 0.794 \\
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}\citep{zurada2014classification}\end{source}}\vspace{-1em}
\end{table}

The same interpretation can be derived if we rank the results in the same way as in Zurada's study, as shown in \autoref{tab:zuradaresultsranks}.
Hence, it is expected that Neural Network would perform well on the HMEQ data set, while Logistic Regression's performance would be unsatisfactory.
\begin{table}[H]
    \small
    \setlength{\tabcolsep}{8pt}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption[Zurada Results - Ranked]{Zurada Results - Ranked}\label{tab:zuradaresultsranks}
    \begin{tabular}{r r r r r r}
    \toprule
    Model & Acc. & Rec. & AUC & Score & Rank \\
    \midrule
    \hline
    NN & 2 & 1 & 1 & 1.33 & 1 \\ 
    DT & 1 & 2 & 2 & 1.67 & 2 \\ 
    SVM & 3 & 3 & 4 & 3.33 & 3 \\ 
    KNN & 5 & 4 & 3 & 4.00 & 4 \\ 
    LR & 4 & 5 & 5 & 4.67 & 5 \\ 
    \hline
    \bottomrule
    \end{tabular}
    \vspace{0.35em}
    
    \centering{\begin{source}Ranking of \citep{zurada2014classification}\end{source}}\vspace{-1em}
\end{table}


\section{Hypotheses}

\noindent \textbf{Hypothesis 1:} The recalibration of the model enhaces model performance \citep{de2023predicting}.
\url{file:///C:/Users/ngnpe/Downloads/Predicting_Readmission_or_Death_After_Discharge_Fr%20(1).pdf}

\textbf{TBD}

\noindent \textbf{Hypothesis 2:} The Neural Network model outperforms all the models.

\textbf{TBD}

\noindent \textbf{Hypothesis 3:} Black--box models perform better than the white--box models.

\textbf{TBD}

\noindent \textbf{Hypothesis 4:} The longer execution time of a model, the better performance \citep{wu2018accurate}.
\url{file:///C:/Users/ngnpe/Downloads/Accurate_Indoor_Localization_Based_on_CSI_and_Visi.pdf}

\textbf{TBD}

\noindent \textbf{Hypothesis 5:} The main default drivers are the debt and/or delinquency variables.

\textbf{TBD}